{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Set up GCP credentials and GCS bucket"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "UsUZobVduL7l"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "# Convert the credentials to a JSON string and set it as an environment variable\n",
        "gcp_path = os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]\n",
        "with open(gcp_path, \"r\") as f:\n",
        "    gcp_json_str = f.read()\n",
        "os.environ[\"DESTINATION__CREDENTIALS\"] =  gcp_json_str \n",
        "\n",
        "# Set the bucket URL as an environment variable\n",
        "os.environ[\"BUCKET_URL\"] = \"gs://ny_taxi_485500_bucket\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Using Terminal, create a virtual environment named .venv and install the required packages for production"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "/opt/homebrew/bin/python3 -m venv .venv \\\n",
        "source .venv/bin/activate \\\n",
        "python -m pip install --upgrade pip setuptools wheel  \n",
        "python -m pip install \\\n",
        "ipykernel \\\n",
        "pandas \\\n",
        "pyarrow \\\n",
        "requests \\\n",
        "duckdb \\\n",
        "gcsfs \\\n",
        "\"dlt[duckdb,gs,bigquery]\" \\\n",
        "\n",
        "#Verify that the system is using .venv as the current kernel to install the pip\n",
        "import sys\n",
        "print(sys.executable)\n",
        "Should now show your venv path\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "lYh7r1mTf4uo"
      },
      "outputs": [],
      "source": [
        "import dlt\n",
        "import requests\n",
        "import pandas as pd\n",
        "from dlt.destinations import filesystem\n",
        "from io import BytesIO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76zT1PzAgs7A"
      },
      "source": [
        "Ingesting parquet files to GCS."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "xya0215jsnsb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pipeline rides_pipeline load step completed in 1 minute and 26.99 seconds\n",
            "1 load package(s) were loaded to destination filesystem and into dataset rides_dataset\n",
            "The filesystem destination used gs://ny_taxi_485500_bucket location to store data\n",
            "Load package 1770667338.531616 is LOADED and contains no failed jobs\n"
          ]
        }
      ],
      "source": [
        "# Define a dlt source to download and process Parquet files as resources\n",
        "@dlt.source(name=\"rides\")\n",
        "def download_parquet():\n",
        "    prefix = \"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata\"\n",
        "    for month in range(1, 7):\n",
        "        url = f\"{prefix}_2024-0{month}.parquet\"\n",
        "        response = requests.get(url)\n",
        "        file_name = f\"yellow_tripdata_2024-0{month}.parquet\"\n",
        "        \n",
        "        df = pd.read_parquet(BytesIO(response.content))\n",
        "\n",
        "        # Return the dataframe as a dlt resource for ingestion\n",
        "        yield dlt.resource(df, name=file_name)\n",
        "\n",
        "\n",
        "# Initialize the pipeline\n",
        "pipeline = dlt.pipeline(\n",
        "    pipeline_name=\"rides_pipeline\",\n",
        "    destination=filesystem(layout=\"{schema_name}/{table_name}.{ext}\"),\n",
        "    dataset_name=\"rides_dataset\",\n",
        ")\n",
        "\n",
        "# Run the pipeline to load Parquet data into DuckDB\n",
        "load_info = pipeline.run(download_parquet(), loader_file_format=\"parquet\")\n",
        "\n",
        "# Print the results\n",
        "print(load_info)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0310FT-gy_P"
      },
      "source": [
        "Ingesting data to Database"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1_3K97w1c2v2",
        "outputId": "4b2d26bf-2814-46fa-f80d-7a2e17417a95"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/huongnguyen/data-engineer-zoomcamp/03_Module_Data_Warehouse/.venv/lib/python3.13/site-packages/google/cloud/bigquery/client.py:613: UserWarning: Cannot create BigQuery Storage client, the dependency google-cloud-bigquery-storage is not installed.\n",
            "  warnings.warn(\n",
            "/Users/huongnguyen/data-engineer-zoomcamp/03_Module_Data_Warehouse/.venv/lib/python3.13/site-packages/google/cloud/bigquery/client.py:613: UserWarning: Cannot create BigQuery Storage client, the dependency google-cloud-bigquery-storage is not installed.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pipeline rides_pipeline load step completed in 1 minute and 56.34 seconds\n",
            "1 load package(s) were loaded to destination bigquery and into dataset rides_dataset\n",
            "The bigquery destination used dtc-de-course@dtc-de-course-485500.iam.gserviceaccount.com@dtc-de-course-485500 location to store data\n",
            "Load package 1770669230.398151 is LOADED and contains no failed jobs\n"
          ]
        }
      ],
      "source": [
        "# Define a dlt resource to download and process Parquet files as single table\n",
        "@dlt.resource(name=\"rides\", write_disposition=\"replace\")\n",
        "def download_parquet():\n",
        "    prefix = 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata'\n",
        "\n",
        "    for month in range(1, 7):\n",
        "        url = f\"{prefix}_2024-0{month}.parquet\"\n",
        "        response = requests.get(url)\n",
        "\n",
        "        df = pd.read_parquet(BytesIO(response.content))\n",
        "\n",
        "        yield df\n",
        "\n",
        "\n",
        "# Initialize the pipeline\n",
        "pipeline = dlt.pipeline(\n",
        "    pipeline_name=\"rides_pipeline\",\n",
        "    #destination=\"duckdb\",  # Use DuckDB for testing\n",
        "    destination=\"bigquery\",  # Use BigQuery for production\n",
        "    dataset_name=\"rides_dataset\",\n",
        ")\n",
        "\n",
        "# Run the pipeline to load Parquet data into DuckDB\n",
        "info = pipeline.run(download_parquet)\n",
        "\n",
        "# Print the results\n",
        "print(info)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gDcLjzLtooBV",
        "outputId": "74ff2de7-2f2e-41b9-a681-3dc5887f6eed"
      },
      "outputs": [],
      "source": [
        "import duckdb\n",
        "\n",
        "conn = duckdb.connect(f\"{pipeline.pipeline_name}.duckdb\")\n",
        "\n",
        "# Set search path to the dataset\n",
        "conn.sql(f\"SET search_path = '{pipeline.dataset_name}'\")\n",
        "\n",
        "# Describe the dataset to see loaded tables\n",
        "res = conn.sql(\"DESCRIBE\").df()\n",
        "print(res)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VVJy8JoerI2P",
        "outputId": "3f8c7fee-a9ee-4fd4-ec75-153ca60bd36f"
      },
      "outputs": [],
      "source": [
        "# provide a resource name to query a table of that name\n",
        "with pipeline.sql_client() as client:\n",
        "    with client.execute_query(f\"SELECT count(1) FROM rides\") as cursor:\n",
        "        data = cursor.df()\n",
        "print(data)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
